:chapnum: 02
:figure-number: 00

== In The Newsroom

image::figs/incoming/02-00-cover.png[float="none",role="informal"]

How does data journalism sit within newsrooms around the world? How did leading data journalists convince their colleagues that it is a good idea to publish datasets or launch data-driven news apps? Should journalists learn how to code, or work in tandem with talented developers? In this section we look at the role of data and data journalism at the Australian Broadcasting Corporation, the BBC, the Chicago Tribune, the Guardian, the Texas Tribune, and the Zeit Online. We learn how to spot and hire good developers, how to engage people around a topic through hackathons and other events, how to collaborate across borders, and business models for data journalism.

=== The ABC's Data Journalism Play 

The Australian Broadcasting Corporation is Australia's national public broadcaster. Annual funding is around $1 billion dollars (AUS), which delivers seven radio networks, 60 local radio stations, three digital television services, a new international television service, and an online platform to deliver this ever-expanding offering of digital and user-generated content. At last count, there were over 4,500 full time equivalent staff, and nearly 70% of them make content.

We are a national broadcaster fiercely proud of our independence; although funded by the government, we are separated at arm's length through law. Our traditions are independent public service journalism. The ABC is regarded as the most trusted news organization in the country.

These are exciting times; under a managing director (the former newspaper executive Mark Scott), content makers at the ABC have been encouraged to be "agile," as the corporate mantra puts it.

Of course, that's easier said than done. 

But one initiative in recent times designed to encourage this has been  a competitive staff pitch for money to develop multiplatform projects.

This is how the ABC's first ever data journalism project was conceived.

Sometime early in 2010, I wandered into the pitch session to face three senior "ideas" people with my proposal.

I'd been chewing it over for some time, greedily lapping up the data journalism that the now legendary Guardian Datablog was offering, and that was just for starters.

It was my argument that no doubt within 5 years, the ABC would have its own data journalism unit. It was inevitable, I opined. But the question was how we would get there, and who was going to start.

For those readers unfamiliar with the ABC, think of a vast bureaucracy built up over 70 years. Its primary offering was always radio and television. With the advent of a website, in the last decade this content offering unfurled into text, stills, and a degree of interactivity previously unimagined. The web space was forcing the ABC to rethink how it cut the cake (money) and what kind of cake it was baking (content).

It is of course a work in progress. 

But something else was happening with data journalism. Government 2.0 (which as we discovered, is largely observed in the breach in Australia) was starting to offer new ways of telling stories that were hitherto buried in the zeros and ones.

I said all of this to the folks attending my pitch. I also said that we needed to identify new skill sets and train journalists in new tools. We needed a project to hit play.

And they gave me the money.

On the 24th of November 2011, the ABC's multiplatform project and ABC News Online went live with http://bit.ly/abc-coal["Coal Seam Gas by the Numbers"].

[[FIG021]]
.Coal Seam Gas by the Numbers (ABC News Online)
image::figs/incoming/02-01.png[float="none"]

It was made up of five pages of interactive maps, data visualizations, and text.

It wasn't exclusively data journalism, but a hybrid of journalisms that was born of the mix of people on the team and the story, which is raging as one of the hottest issues in Australia.

The jewel was an interactive map showing coal seam gas wells and leases in Australia. Users could search by location and switch between modes to show leases or wells. By zooming in, users could see who the explorer was, the status of the well, and its drill date. Another map showed the location of coal seam gas activity compared to the location of groundwater systems in Australia.

[[FIG023]]
.Interactive map of gas wells and leases in Australia (ABC News Online)
image::figs/incoming/02-02.png[float="none"]

We had data visualizations that specifically addressed the issue of waste salt and water production that would be generated depending on the scenario that emerged.

Another section of the project investigated the release of chemicals into a local river system.

==== Our Team

  * A web developer and designer
  * A lead journalist  
  * A part-time researcher with expertise in data extraction, Excel spreadsheets, and data cleaning
  * A part-time junior journalist
  * A consultant executive producer
  * A academic consultant with expertise in data mining, graphic visualization, and advanced research skills
  * The services of a project manager and the administrative assistance of the ABC's multiplatform unit
  * Importantly, we also had a reference group of journalists and others whom we consulted on an as-needed basis

==== Where Did We Get the Data From?

The data for the interactive maps were scraped from shapefiles (a common kind of file for geospatial data) downloaded from government websites. 

Other data on salt and water were taken from a variety of reports.

The data on chemical releases was taken from environmental permits issued by the government. 

==== What Did We Learn?

"Coal Seam Gas by the Numbers" was ambitious in content and scale.  Uppermost in my mind was what did we learn, and how might we do it differently next time?

The data journalism project brought a lot of people into the room who do not normally meet at the ABC: in lay terms, the hacks and the hackers. Many of us did not speak the same language or even appreciate what the other group did.  Data journalism is disruptive!

The practical things:

  * Co-location of the team is vital. Our developer and designer were offsite and came in for meetings. This is definitely not optimal! Place everyone in the same room as the journalists.
  * Our consultant EP was also on another level of the building. We needed to be much closer, just for the drop-by factor.
  * Choose a story that is solely data-driven.

==== The Big Picture: Some Ideas

Big media organzations need to engage in capacity building to meet the challenges of data journalism. My hunch is that there are a lot of geeks and hackers hiding in media technical departments desperate to get out. So we need "hack and hacker meets" workshops where the secret geeks, younger journalists, web developers, and designers come out to play with more experienced journalists for skill-sharing and mentoring. Task: download this dataset and go for it!

Ipso facto, data journalism is interdisciplinary. Data journalism teams are made of people who would not have worked together in the past. The digital space has blurred the boundaries.

We live in a fractured, distrustful body politic. The business model that formerly delivered professional independent journalism--imperfect as it is--is on the verge of collapse. We ought to ask ourselves, as many now are, what might the world look like without a viable fourth estate? The American journalist and intellectual Walter Lippman remarked in the 1920’s that ``it is admitted that a sound public opinion cannot exist without access to news.'' That statement is no less true now. In the 21st century, everyone’s hanging out in the blogosphere. It’s hard to tell the spinners, liars, dissemblers, and vested interest groups from the professional journalists. Pretty much any site or source can be made to look credible, slick, and honest. The trustworthy mastheads are dying in the ditch. And in this new space of junk journalism, hyperlinks can endlessly take the reader to other more useless but brilliant-looking sources that keep hyperlinking back into the digital hall of mirrors. The technical term for this is: bullshit baffles brains.

In the digital space, everyone’s a storyteller now, right? Wrong. If professional journalism--and by that I mean those who embrace ethical, balanced, courageous truth-seeking storytelling--is to survive, then the craft must reassert itself in the digital space. Data journalism is just another tool by which we will navigate the digital space. It’s where we will map, flip, sort, filter, extract, and see the story amidst all those zeros and ones. In the future we’ll be working side by side with the hackers, the developers, the designers, and the coders. It’s a transition that requires serious capacity building. We need news managers who ``get'' the digital/journalism connection to start investing in the build.

&mdash; _Wendy Carlisle, Australian Broadcasting Corporation_

=== Data Journalism at the BBC

The term "data journalism" can cover a range of disciplines and is used in varying ways in news organizations, so it may be helpful to define what we mean by "data journalism" at the BBC. Broadly, the term covers projects that use data to do one or more of the following:

  * Enable a reader to discover information that is personally relevant
  * Reveal a story that is remarkable and previously unknown
  * Help the reader to better understand a complex issue

These categories may overlap, and in an online environment, can often benefit from some level of visualization.

==== Make It Personal

On the BBC News website, we have been using data to provide services and tools for our users for well over a decade.

The most consistent example, which we first published in 1999, is our http://bbc.in/school-league-tables[school league tables], which use the data published annually by the government. Readers can find local schools by entering a postcode, and compare them on a range of indicators. Education journalists also work with the development team to trawl the data for stories ahead of publication.

When we started to do this, there was no official site that provided a way for the public to explore the data. But now that the Department for Education has its own comparable service, our offering has shifted to focus more on the stories that emerge from the data.

The challenge in this area must be to provide access to data in which there is a clear public interest. A recent example of a project where we exposed a large dataset not normally available to the wider public was the special report http://bbc.in/road-deaths["Every death on every road"]. We provided a postcode search, allowing users to find the location of all road fatalities in the UK in the past decade.

We http://bbc.in/police-data[visualized some of the main facts and figures] that emerge from the police data and, to give the project a more dynamic feel and a human face, we teamed up with the London Ambulance Association and BBC London radio and TV to track crashes across the capital as they happened. This was reported http://bbc.in/road-deaths-feed[live online], as well as via Twitter using the hashtag #crash24, and collisions were http://bbc.in/road-deaths-map[mapped] as they were reported.

==== Simple Tools

As well as providing ways to explore large datasets, we have also had success creating simple tools for users that provide personally relevant snippets of information. These tools appeal to the time-poor, who may not choose to explore lengthy analysis. The ability to easily share a personal fact is something we have begun to incorporate as standard.

A lighthearted example of this approach is our feature http://bbc.in/KQsSzB["The world at 7 billion: What's your number?"], published to coincide with the official date at which the world's population exceeded 7 billion. By entering their birth date, the user could find out what "number" they were, in terms of the global population, when they were born and then share that number via Twitter or Facebook. The application used data provided by the UN population development fund. It was very popular, and became the most shared link on Facebook in the UK in 2011.

[[FIG024]]
.The world at seven billion (BBC)
image::figs/incoming/02-05.png[float="none"]

Another recent example is the BBC http://bbc.in/JepssY[budget calculator], which enabled users to find out how better or worse off they will be when the Chancellor's budget takes effect--and then share that figure. We teamed up with the accountancy firm KPMG LLP, who provided us with calculations based on the annual budget, and then we worked hard to create an appealing interface that would encourage users to complete the task.

==== Mining The Data

But where is the journalism in all this? Finding stories in data is a more traditional definition of data journalism. Is there an exclusive buried in the database? Are the figures accurate? Do they prove or disprove a problem? These are all questions a data journalist or computer-assisted reporter must ask themselves. But a great deal of time can be taken up sifting through a massive dataset in the hope of finding something remarkable.

In this area, we have found it most productive to partner with investigative teams or programs that have the expertise and time to investigate a story. The BBC current affairs program Panorama spent months working with the Centre for Investigative Journalism, gathering data on public sector pay. The result was a TV documentary and a special report online, http://bbc.in/IKPrL2["Public Sector pay: The numbers"], where all the data was published and visualized with sector by sector analysis.

As well as partnering with investigative journalists, having access to numerous journalists with specialist knowledge is essential. When a business colleague on the team analyzed the spending review cuts data put out by the government, he came to the conclusion that it was making them sound bigger than they actually were. The result was an exclusive story, http://bbc.in/LcuGFV["Making sense of the data"] complemented by a http://bbc.in/IIADrj[clear visualization], which won a Royal Statistical Society award.

==== Understanding An Issue

But data journalism doesn't have to be an exclusive no one else has spotted. The job of the data visualization team is to combine great design with a clear editorial narrative to provide a compelling experience for the user. Engaging visualizations of the right data can be used to give a better understanding of an issue or story, and we frequently use this approach in our storytelling at the BBC. One technique used in our UK claimant count tracker is to http://bbc.in/KF7IKU[heat-map data] over time to give a clear view of change.

The data feature http://bbc.in/IIAHHI["Eurozone debt web"] explores the tangled web of intra-country lending. It helps to explain a complicated issue in a visual way, using color and proportional arrows combined with clear text. An important consideration is to encourage the user to explore the feature or follow a narrative, without making him feel overwhelmed by the numbers.

==== Team Overview

The team that produces data journalism for the BBC News website is comprised of about 20 journalists, designers, and developers.

As well as data projects and visualizations, the team produces all the infographics and interactive multimedia features on the news website. Together, these form a collection of storytelling techniques we have come to call _visual journalism_. We don't have people who are specifically identified as data journalists, but all editorial staff on the team have to be proficient at using basic spreadsheet applications such as Excel and Google Docs to analyze data.

Central to any data projects are the technical skills and advice of our developers and the visualization skills of our designers. While we are all either a journalist, designer, or developer "first," we continue to work hard to increase our understanding and proficiency in each other's areas of expertise.

The core products for exploring data are Excel, Google Docs, and Fusion Tables. The team has also, but to a lesser extent, used MySQL, Access databases, and Solr to explore larger datasets; and used RDF and SPARQL to begin looking at ways in which we can model events using Linked Data technologies. Developers will also use their programming language of choice, whether that's ActionScript, Python, or Perl, to match, parse, or generally pick apart a dataset we might be working on. Perl is used for some of the publishing.

We use Google, Bing Maps, and Google Earth, along with Esri's ArcMAP, for exploring and visualizing geographical data.

For graphics we use the Adobe Suite including After Effects, Illustrator, Photoshop, and Flash, although we would rarely publish Flash files on the site these days as JavaScript--particularly JQuery and other JavaScript libraries like Highcharts, Raphael and D3--increasingly meets our data visualization requirements.

&mdash; _Bella Hurrell and Andrew Leimdorfer, BBC_

=== How the News Apps Team at the Chicago Tribune Works

The news applications team at the Chicago Tribune is a band of happy hackers embedded in the newsroom. We work closely with editors and reporters to help: 1) research and report stories, 2) illustrate stories online and 3) build evergreen web resources for the fine people of Chicagoland.

It's important that we sit in the newsroom. We usually find work via face-to-face conversations with reporters. They know that we're happy to help write a screen scraper for a crummy government website, tear up a stack of PDFs, or otherwise turn non-data into something you can analyze. It's sort of our team's loss leader; this way we find out about potential data projects at the outset.

Unlike many teams in this field, our team was founded by technologists for whom journalism was a career change. Some of us acquired a masters degree in journalism after several years of coding for business purposes, and others were borrowed from the open government community.

We work in an agile fashion. To make sure we're always in sync, every morning begins with a 5-minute stand-up meeting. We frequently program in pairs; two developers at one keyboard are often more productive than two developers at two keyboards. Most projects don't take more than a week to produce, but on longer projects we work in week-long iterations, and show our work to stakeholders (reporters and editors usually) every week. ``Fail fast'' is the mantra. If you're doing it wrong, you need to know as soon as possible, especially when you're coding on a deadline!

There's a tremendous upside to hacking iteratively, on a deadline: we're always updating our toolkit. Every week we crank out an app or two, then, unlike normal software shops, we can put it to the back of our mind and move on to the next project. It's a joy we share with the reporters, and every week we learn something new.

[[FIG025]]
.The Chicago Tribune news applications team (photo by Heather Billings)
image::figs/incoming/02-00.jpg[float="none"]

All app ideas come from the reporters and editors in the newsroom. This, I believe, sets us apart from app teams in other newsrooms, who frequently spawn their own ideas. We've built strong personal and professional relationships in the newsroom, and folks know that when they have data, they come to us.

Much of our work in the newsroom is reporter support. We help reporters dig through data, turn PDFs back into spreadsheets, screen-scrape websites, etc. It's a service that we like to provide because it gets us in early on the data work that's happening in the newsroom. Some of that work becomes a news application: a map, table, or sometimes a larger-scale website.

Before, we linked to the app from the written story, which didn't result in much traffic. These days apps run near the top of our website, and the app links through to the story, which works nicely for both the app and the story. There is a http://www.chicagotribune.com/news/data/[section of the website for our work], but it's not well-trafficked. But that's not surprising. ``Hey, today I want some data!,'' isn't a very big use case.

We love page views, and we love the accolades of our peers, but that's weak sauce. The motivation should always be impact; on people's lives, on the law, on holding politicians to account, and so on. The written piece will speak to the trend and humanize it with a few anecdotes. But what's the reader to do when they've finished the story? Is their family safe? Are their children being educated properly? Our work sings when it helps a reader find his or her _own_ story in the data. Examples of impactful, personalized work that we've done include our http://nursinghomes.apps.chicagotribune.com/[Nursing Home Safety Reports] and http://schools.chicagotribune.com/[School Report Card] apps.

&mdash; _Brian Boyer, Chicago Tribune_ 

=== Behind the Scenes at the Guardian Datablog 

When we launched the Datablog, we had no idea who would be interested in raw data, statistics, and visualizations. As someone rather senior in my office said, ``why would anyone want that?''

The http://www.guardian.co.uk/datablog[Guardian Datablog], which I edit, was to be a small blog offering the full datasets behind our news stories. Now it consists of a http://guardian.co.uk/data[front page]; searches of world government and global development data; data visualizations by Guardian graphic artists and from around the Web, and tools for exploring public spending data. Every day, we use Google spreadsheets to share the full data behind our work; we visualize and analyze that data, and then use it to provide stories for the newspaper and the site.

As a news editor and journalist working with graphics, it was a logical extension of work I was already doing, accumulating new datasets and wrangling with them to try to make sense of the news stories of the day.

The question I was asked has been answered for us. It has been an incredible few years for public data. Obama opened up the US government's data vaults as his first legislative act, and his example was soon followed by government data sites around the world: Australia, New Zealand, and the British government's Data.gov.uk.

We've had the MPs expenses scandal, Britain's most unexpected piece of data journalism--the resulting fallout has meant that Westminster is now committed to releasing huge amounts of data every year.

We had a general election where each of the main political parties was committed to data transparency, opening our own data vaults to the world. We've had newspapers devoting valuable column inches to the release of the Treasury's COINS database.

At the same time, as the Web pumps out more and more data, readers from around the world are more interested in the raw facts behind the news than ever before. When we launched the Datablog, we thought the audiences would be developers building applications. In fact, it's people wanting to know more about carbon emissions, Eastern European immigration, the breakdown of deaths in Afghanistan, or even the number of times the Beatles used the word ``love'' in their songs (613).

[[FIG026]]
.The Guardian Datablog production process visualized (the Guardian)
image::figs/incoming/02-ZZ.jpg[float="none"]

++++
<?dbfo-need height="1in"?>
++++

Gradually, the Datablog's work has reflected and added to the stories we faced. We crowdsourced 458,000 documents relating to MPs' expenses and we analyzed the detailed data of which MPs had claimed what. We helped our users explore detailed Treasury spending databases and published the data behind the news.

But the game-changer for data journalism happened in spring 2010, beginning with one spreadsheet: 92,201 rows of data, each one containing a detailed breakdown of a military event in Afghanistan. This was the WikiLeaks war logs. Part one, that is. There were to be two more episodes to follow: Iraq and the cables. The official term for the first two parts was SIGACTS: the US military Significant Actions Database.

News organizations are all about geography--and proximity to the news desk. If you're close, it's easy to suggest stories and become part of the process; conversely, out of sight is literally out of mind. Before WikiLeaks, we were placed on a different floor, with graphics. Since WikiLeaks, we have sat on the same floor, next to the newsdesk. It means that it's easier for us to suggest ideas to the desk, and for reporters across the newsroom to think of us to help with stories.

It's not that long ago journalists were the gatekeepers to official data. We would write stories about the numbers and release them to a grateful public, who were not interested in the raw statistics. The idea of us allowing raw information into our newspapers was anathema.

Now that dynamic has changed beyond recognition. Our role is becoming interpreters; helping people understand the data, and even just publishing it because it's interesting in itself.

But numbers without analysis are just numbers, which is where we fit in. When Britain's prime minister claims the riots in August 2011 were not about poverty, we were able to map the addresses of the rioters with poverty indicators to show the truth behind the claim.

Behind all of our data journalism stories is a process. It's changing all the time as we use new tools and techniques. Some people say the answer is to become a sort of super hacker, write code, and immerse yourself in SQL. You can decide to take that approach. But a lot of the work we do is just in Excel.

Firstly, we locate the data or receive it from a variety of sources, from breaking news stories, government data, journalists' research and so on. We then start looking at what we can do with the data; do we need to mash it up with another dataset? How can we show changes over time? Those spreadsheets often have to be seriously tidied up--all those extraneous columns and weirdly merged cells really don't help. And that's assuming it's not a PDF, the worst format for data known to humankind.

Often official data comes with the official codes added in; each school, hospital, constituency, and local authority has a unique identifier code.

++++
<?dbfo-need height="1in"?>
++++

Countries have them too (the UK's code is GB, for instance). They're useful because you may want to start mashing datasets together, and it's amazing how many different spellings and word arrangements can get in the way of that. There's Burma and Myanmar, for instance, or Fayette County in the US (there are 11 of these in states from Georgia to West Virginia). Codes allow us to compare like with like.

At the end of that process is the output: will it be a story or a graphic or a visualization, and what tools will we use? Our top tools are the free ones that we can produce something quickly with. The more sophisticated graphics are produced by our dev team.

This means that we commonly use Google charts for small line graphs and pies, or Google Fusion Tables to create maps quickly and easily.

It may seem new, but it's really not.

In the very first issue of the Manchester Guardian (on Saturday May 5th, 1821), the news was on the back page, like all papers of the day. The first item on the front page was an ad for a missing Labrador.

Amid the stories and poetry excerpts, a third of that back page is taken up with, well, facts. A comprehensive table of the costs of schools in the area never before "laid before the public," writes "NH."

NH wanted his data published because otherwise the facts would be left to untrained clergymen to report. His motivation was that, ``Such information as it contains is valuable; because, without knowing the extent to which education … prevails, the best opinions which can be formed of the condition and future progress of society must be necessarily incorrect.'' In other words, if the people don't know what's going on, how can society get any better?

I can't think of a better rationale now for what we're trying to do. Now what once was a back page story can now make front page news.

&mdash; _Simon Rogers, the Guardian_

=== Data Journalism at the Zeit Online

The http://bit.ly/Pisa_Wealth[PISA based Wealth Comparison] project is an interactive visualization that enables comparison of standards of living in different countries. It uses data from the OECD's comprehensive world education ranking report, http://bit.ly/Pisa_2009[PISA 2009], published in December 2010. The report is based on a questionnaire which asks fifteen year-old pupils about their living situation at home.

The idea was to analyze and visualize this data to provide a unique way of comparing standards of living in different countries.

[[FIG027]]
.PISA based Wealth Comparison (Zeit Online)
image::figs/incoming/02-03-AA.png[float="none"]

First, our in-house editorial team decided which facts seemed to be useful to make living standards comparable and should be visualized, including:

  * Wealth (number of TVs, cars and available bathrooms at home)
  * Family situation (whether there are grandparents living with the family, percentage share of families with only one child, unemployment of parents, and mother's job status)
  * Access to knowledge sources (Internet at home, frequency of using email and quantity of owned books)
  * Three additional indicators on the level of development of each country

With the help of the internal design team, these facts were translated into self-explanatory icons. A front end design was built to make comparison between the different countries possible, by looking at them like they were playing cards.

Next we contacted people from the German http://opendata-network.org/[Open Data Network] to find developers who could help with the project. This community of highly motivated people suggested Gregor Aisch, a very talented information designer, to code the application that would make our dreams come true (without using Flash, which was very important to us!). Gregor created a very high quality and interactive visualization with a beautiful bubble-style, based on the http://raphaeljs.com/[Raphaël-Javascript Library].

The result of our collaboration was a very successful interactive that got a lot of traffic. It is easy to compare any two countries, which makes it useful as a reference tool. This means that we can reuse it in our daily editorial work. For example, if we are covering something related to the living situation in Indonesia, we can quickly and easily embed a graphic http://bit.ly/Pisa_Indonesia_Germany[comparing the living situation in Indonesia and Germany]. The know-how transferred to our in-house team was a great investment for future projects.

At the Zeit Online, we've found that http://www.zeit.de/datenjournalismus[our data journalism projects] have brought us a lot of traffic and have helped us to engage audiences in new ways. For example, there was a lot of coverage about the situation at the nuclear plant in Fukushima after the tsunami in Japan. After radioactive material escaped from the power plant, everyone within 30 kilometers of the plant was evacuated. People could read and see a lot about the evacuations. Zeit Online found a innovative way to explain the impact of this to our German audience. We asked: how many people live near a nuclear power plant in Germany? How many people live within a radius of 30 kilometers? A map shows http://bit.ly/near_nuclear[how many people would have to be evacuated in a similar situation in Germany]. The result: lots and lots of traffic; in fact, the project went viral in the social media sphere. Data journalism projects can be relatively easily adapted to other languages. We created an English language version about proximity to nuclear power plants in the US, which was a great traffic motor. News organizations want to be recognized as trusted and authoritative sources amongst their readers.  We find that data journalism projects combined with enabling our readers to look and reuse the raw data brings us a high degree of credibility.

For two years the R&D Department and the Editor-in-Chief at the Zeit Online, Wolfgang Blau, have been advocating data journalism as an important way to tell stories. Transparency, credibility, and user engagement are important parts of our philosophy. That is why data journalism is a natural part of our current and future work. Data visualizations can bring value to the reception of a story, and are an attractive way for the whole editorial team to present their content.

For example, on November 9th, 2011, Deutsche Bank pledged to stop financing cluster bomb manufacturers. But according to a study by non-profit organization Facing Finance, the bank continued to approve loans to producers of cluster munitions after that promise was made. http://zeit.de/wirtschaft/cluster-munition[Our visualization] based on the data shows our readers the various flows of money. The different parts of the Deutsche Bank company are arranged at the top, with the companies accused of involvement in building cluster munitions at the bottom. In between, the individual loans are represented along a timeline. Rolling over the circles shows the details of each transaction. Of course the story could have been told as a written article. But the visualization enables our readers to understand and explore the financial dependencies in a more intuitive way.

[[FIG028]]
.The business of bombs (Zeit Online)
image::figs/incoming/02-03-DD.png[float="none"]

To take another example: the https://www.destatis.de/EN/Homepage.html[German Federal Statistic Office] has published a great dataset on vital statistics for Germany, including http://bit.ly/German_Federal_Statistics[modeling various demographic scenarios up until 2060]. The typical way to represent this is a population pyramid, such as https://www.destatis.de/bevoelkerungspyramide/[the one from the Federal Statistics Agency].

With our colleagues from the science department, we tried to give our readers a better way to explore the projected demographic data about our future society. With http://www.zeit.de/wissen/altersstruktur[our visualization], we present a statistically representative group of 40 people of different ages from the years 1950 till 2060.They are organized into eight different groups. It looks like a group photo of German society at different points in time. The same data visualized in a traditional population pyramid gives only a very abstract feeling of the situation, but a group with kids, younger people, adults, and elderly people means our readers can relate to the data more easily. You can just hit the play button to start a journey through eleven decades. You can also enter your own year of birth and gender to become part of the group photo: to see your demographic journey through the decades and your own life expectancy.

&mdash; _Sascha Venohr, Zeit Online_

[[FIG0210]]
.Visualizing demographic data (Zeit Online)
image::figs/incoming/02-03-CC.png[scale="94",float="none"]

=== How to Hire a Hacker

One of the things that I am regularly asked by journalists is "how do I get a coder to help me with my project?" Don't be deceived into thinking that this is a one-way process; civic-minded hackers and data-wranglers are often just as keen to get in touch with journalists.

Journalists are power-users of data driven tools and services. From the perspective of developers, journalists think outside the box to use data tools in contexts developers haven't always considered before (feedback is invaluable!). They also help to build context and buzz around projects and help to make them relevant. It is a symbiotic relationship.

Fortunately, this means that whether you are looking to hire a hacker or looking for possible collaborations on a shoestring budget, there will more than likely be someone out there who is interested in helping you.

So how do you find them? Says Aron Pilhofer from The New York Times:

[quote]
____
You may find that your organzation already has people with all the skills you need, but they are not necessarily already in your newsroom. Wander around, visit the technology and IT departments, and you are likely to strike gold. It is also important to appreciate coder culture: come across someone who has a computer that looks like the one in <<FIG0211>>...then you are probably onto a winner.
____

[[FIG0211]]
.Badge of honor: hackers are often easy to spot (photo by Lucy Chambers)
image::figs/incoming/02-04.jpg[float="none"]

Here are a few more ideas:

Post on job websites::
  Identify and post to websites aimed at developers who work in different programming languages. For example, the http://www.python.org/community/jobs/[Python Job Board].
Contact relevant mailing lists::
  For example, the http://bit.ly/nicar-subscribe[NICAR-L] and http://bit.ly/ddj-list[Data Driven Journalism] mailing lists.
Contact relevant organizations::
  For example, if you want to clean up or scrape data from the web, you could contact an organization such as https://scraperwiki.com/[Scraperwiki], who have a great address book of trusted and willing coders.
Join relevant groups/networks::
  Look out for initiatives such as http://hackshackers.com/[Hacks/Hackers] which bring journalists and techies together. Hacks/Hackers groups are now springing up all around the world. You could also try posting something to their http://bit.ly/hacks-hackers-jobs[jobs newsletter].
Local interest communities::
  You could try doing a quick search for an area of expertise in your area (e.g. "javascript" + "london"). Sites such as Meetup.com can also be a great place to start.
Hackathons and competitions::
  Whether or not there is prize money available, app and visualization competitions and development days are often fruitful ground for collaboration and making connections.
Ask a geek!::
  Geeks hang around with other geeks. Word of mouth is always a good way to find good people to work with.

&mdash; _Lucy Chambers, Open Knowledge Foundation_

.Hacker Skills
****
Once you've found a hacker, how do you know if they are any good? We asked Alastair Dant from the Guardian for his views on how to spot a good one:

They code the full stack::
  When dealing with deadlines, it's better to be a jack of all trades than a master of one. News apps require data wrangling, dynamic graphics, and derring-do.
They see the whole picture::
  Holistic thinking favors narrative value over technical detail. I'd rather hear one note played with feeling than unceasing virtuosity in obscure scales. Find out how happy someone is to work alongside a designer.
They tell a good story::
  Narrative presentation requires arranging things in space and time. Find out what project they're most proud of and ask them to walk you through how it was built; this will reveal as much about their ability to communicate as their technical understanding.
They talk things through::
  Building things fast requires mixed teams working towards common goals. Each participant should respect their fellows and be willing to negotiate. Unforeseen obstacles often require rapid re-planning and collective compromise.
They teach themselves::
  Technology moves fast. It's a struggle to keep up with. Having met good developers from all sorts of backgrounds, the most common trait is a willingness to learn new stuff on demand.

&mdash; _Lucy Chambers, Open Knowledge Foundation, interviewing Alastair Dant, Lead pass:[<phrase role='keep-together'>Interactive</phrase>] Technologist, the Guardian_
****

++++
<?dbfo-need height="1in"?>
++++

.How To Find Your Dream Developer
****
The productivity difference between a good and a great developer is not linear--it's exponential. Hiring well is extremely important. Unfortunately, hiring well is also very difficult. It's hard enough to vet candidates if you are not an experienced technical manager. Add to that the salaries that news organizations can afford to pay, and you've got quite a challenge.

At Tribune, we recruit with two angles: an emotional appeal and a technical appeal. The emotional appeal is this: journalism is essential to a functioning democracy. Work here and you can change the world. Technically, we promote how much you'll learn. Our projects are small, fast, and iterative. Every project is a new set of tools, a new language, a new topic (fire safety, the pension system), that you must learn. The newsroom is a crucible. I've never managed a team that has learned so much, so fast, as our team.

As for where to look, we've had great luck finding great hackers in the open government community. The Sunlight Labs mailing list is where do-gooder nerds with crappy day jobs hang out at night. Another potential resource is Code for America. Every year, a group of fellows emerges from CfA, looking for their next big project. And as a bonus, CfA has a rigorous interview process; they've already done the vetting for you. Nowadays, programming-interested journalists are also emerging from journalism schools. They're green, but they've got tons of potential.

Lastly, it's not enough to just hire developers. You need technical management. A lone-gun developer (especially fresh from journalism school, with no industry experience) is going to make many bad decisions. Even the best programmer, when left to her own devices, will choose technically interesting work over doing what's most important to your audience.

Call this hire a news applications editor, a project manager, whatever. Just like writers, programmers need editors, mentorship, and somebody to wrangle them towards making software on deadline.

&mdash; _Brian Boyer, Chicago Tribune_
****


=== Harnessing External Expertise Through Hackathons

In March 2010, Utrecht-based digital culture organzation SETUP put on an event called http://setup.nl/content/hacking-journalism[Hacking Journalism]. The event was organized to encourage greater collaboration between developers and journalists.

"We organize hackathons to make cool applications, but we can't recognize interesting stories in data. What we build has no social relevance," said the programmers. "We recognize the importance of data journalism, but we don't have all the technical skills to build the things we want," said the journalists.

[[FIG0212]]
.Journalists and developers at RegioHack (photo by Heinze Havinga)
image::figs/incoming/02-XY.jpg[float="none"]

Working for a regional newspaper, there was no money or incentive to hire a programmer for the newsroom. Data journalism was still an unknown quantity for Dutch newspapers at that time.

The hackathon model was perfect; a relaxed environment for collaboration, with plenty of pizza and energy drinks. http://www.regiohack.nl/[RegioHack] was a hackathon organized by my employer, the regional newspaper http://www.destentor.nl/[De Stentor], our sister publication http://www.tctubantia.nl/[TC Tubantia], and http://saxion.nl/[Saxion Hogescholen Enschede], who provided the location for the event.

The setup was as follows: everyone could enlist for a 30-hour hackathon. We provided the food and drink. We aimed for 30 participants, which we divided into 6 groups. These groups would focus on different topics, such as crime, health, transport, safety, aging, and power. For us, the three main objectives for this event were as follows:

Find stories::
  For us, data journalism is something new and unknown. The only way we can prove its use is through well crafted stories. We planned to produce at least three data stories.

++++
<?dbfo-need height="1in"?>
++++

Connect people::
  We, the journalists, don't know how data journalism is done and we don't pretend to. By putting journalists, students, and programmers in one room for 30 hours, we want them to share knowledge and insights.

Host a social event::
  Newspapers don't organize a lot of social events, let alone hackathons. We wanted to experience how such an event can yield results. In fact, the event could have been tense: 30 hours with strangers, lots of jargon, bashing your head against basic questions, and working out of your comfort zone. By making it a social event (remember the pizza and energy drinks?), we wanted to create an environment in which journalists and programmers could feel comfortable and collaborate effectively.

Before the event, TC Tubantia had an interview with the widow of a policeman who had written a book on her husband's working years. She also had a document with all registered murders in the eastern part of the Netherlands, maintained by her husband since 1945. Normally, we would publish this document on our website. This time, we made a http://bit.ly/tableau-dashboard[dashboard using the Tableau software]. We also http://bit.ly/regiohack-blog[blogged] about how this came together on our RegioHack site.

During the hackathon, one project group came up with the subject of development of schools and the aging of our region. http://bit.ly/tableau-workbook[By making a visualization of future projections], we understood which cities would get in trouble after a few years of decline in enrollments. With this insight, we made an article on how this would affect schools in our region.

We also started a very ambitious project called De Tweehonderd van Twente (in English, The Two Hundred of Twente) to determine who had the most power in our region and build a database of the most influential people. Through a Google-ish calculation--who has the most ties with powerful organizations--a list of influential people will be composed. This could lead to a series of articles, but it's also a powerful tool for journalists. Who has connections with who? You can ask questions to this database and use it in your daily routine. Also, this database has cultural value. Artists already asked if they could use this database when finished, in order to make interactive art installations.

[[FIG0213]]
.New communities around data journalism (photo by Heinze Havinga)
image::figs/incoming/02-YY.jpg[float="0"]

After RegioHack, we noticed that journalists considered data journalism a viable addition to traditional journalism. My colleagues continued to use and build on the techniques learned on that day to create more ambitious and technical projects, such as a database of the administrative costs of housing. With this data, I made http://bit.ly/stentor-map[an interactive map in Fusion Tables]. We asked our readers to play around with the data and crowdsourced results at http://bit.ly/scratchbook-crowdsourcing, for example. After a lot of questions on how we made a map in Fusion Tables, I also recorded a http://bit.ly/vermanen-video[video tutorial].

++++
<?dbfo-need height="1in"?>
++++

What did we learn? We learned a lot, but we also came along a lot of obstacles. We recognized these four:

Where to begin: question or data?::
  Almost all projects stalled when searching for information. Most of the time, they began with a journalistic question. But then? What data is available? Where can you find it? And when you find this data, can you answer your question with it? Journalists usually know where they can find information when doing research for an article. With data journalism, most journalists don't know what information is available.

Little technical knowledge::
  Data journalism is quite a technical discipline. Sometimes you have to scrape, other times you'll have to do some programming to visualize your results. For excellent data journalism, you'll need two aspects: the journalistic insight of an experienced journalist and the technical know-how of a digital all-rounder. During RegioHack, this was not a common presence.

Is it news?::
  Participants mostly used one dataset to discover news, instead of searching interconnections between different sources. The reason for this is that you need some statistical knowledge to verify news from data journalism.

What's the routine?::
  What everything above comes down to is that there's no routine. The participants have some skills under their belt, but don't know how and when to use them. One journalist compared it with baking a cake. "We have all the ingredients: flour, eggs, milk, etcetera. Now we throw it all in a bag, shake it, and hope a cake comes out." Indeed, we have all the ingredients, but don't know what the recipe is.

What now? Our first experiences with data journalism could help other journalists or programmers aspiring to enter the same field of work, and we are working to produce a report.

We are also considering how to continue RegioHack in a hackathon form. We found it fun, educational, and productive and a great introduction to data journalism.

But for data journalism to work, we have to integrate it in the newsroom. Journalists have to think in data, in addition to quotes, press releases, council meetings, and so on. By doing RegioHack, we proved to our audience that data journalism isn't just hype. We can write better informed and more distinctive articles, while presenting our readers with different articles in print and online.

&mdash; _Jerry Vermanen, NU.nl_

=== Following the Money: Data Journalism and Cross-Border Collaboration ===

Investigative journalists and citizens interested in uncovering organized crime and corruption that affect the lives of billions worldwide, with each passing day, have unprecedented access to information. Huge volumes of data are made available online by governments and other organizations, and it seems that much needed information is more and more in everyone's grasp. However, at the same time, corrupt officials in governments and organized crime groups are doing their best to conceal information in order to hide their misdeeds. They make efforts to keep people in the dark while conducting ugly deals that cause disruptions at all society levels and lead to conflict, famine, or other crises.

It is the duty of investigative journalists to expose such wrongdoings, and by doing so, disable corrupt and criminal mechanisms.

[[FIG0214]]
.The Investigative Dashboard (OCCRP)
image::figs/incoming/02-RR.png[scale="92",float="0"]

There are three main guidelines that, if followed, can lead to good, thorough journalism when investigating major acts of corruption and crime even in the most austere of environments:

Think outside your country::
  In many instances, it is much easier to get information from abroad than from within the country where the investigative journalist operates. Information gathered from abroad via foreign information databases or by using other countries' access to information laws might be just what you need to put the investigative puzzle together. On top of that, criminals and corrupt officials don't keep their money in the place they have stolen it from. They would rather deposit it in foreign banks or invest in other countries. Crime is global. Databases that assist the investigative journalist in tracking the money worldwide can be found in many places on the Internet. For example, the http://www.investigativedashboard.org/category/wwd/[Investigative Dashboard] enables journalists to follow the money across borders.

Make use of existing investigative journalism networks::
  Investigative journalists all over the world are grouped in
organzations such as http://www.reportingproject.net/[The Organized Crime and Corruption Reporting Project], http://www.fairreporters.org/[The African Forum for Investigative
Reporting], http://arij.net/[The Arab Reporters for Investigative
Journalism], and http://www.gijn.org/[The Global investigative Journalism Network]. Journalists can also make use of professional journalism platforms such as IJNet, where global journalism related information is exchanged on a daily basis. Many of the reporters grouped in networks work on similar issues and confront similar situations, so it makes a lot of sense to exchange information and methods. Emailing lists or social network groups are attached to these networks, so it is quite easy to get in touch with fellow journalists and to ask for information or advice. Investigative story ideas can also be gathered from such forums and emailing lists.

Make use of technology and collaborate with hackers::
  Software helps investigative journalists access and process information. Various types of software assist the investigator in cutting through the noise, in digging and making sense of large volumes of data, and in finding the right documents needed to break the story. There are many ready-made software programs that can be used as tools for analyzing, gathering, or interpreting information--and more importantly, investigative journalists need to be aware that there are scores of computer programmers ready to help if asked. These programmers or hackers know how to obtain and handle information, and they can assist a great deal with the investigative effort. These programmers, some of them members of global open data movements, can become invaluable allies in the fight against crime and corruption, able to assist journalists in gathering and analyzing information.

A good example of an interface between programmers and citizens is https://scraperwiki.com/[ScraperWiki], a site where journalists
can ask programmers for help with extracting data from websites. Investigative Dashboard http://bit.ly/dashboard-resources[maintains a list of ready-made tools] that could help journalists gather, shape, and analyze data.

The usefulness of the aforementioned guidelines has been visible in many instances. One good example is the work of Khadija Ismayilova, a very experienced Azeri investigative reporter who works in an austere environment when it comes to information access. Ms. Ismayilova has to overcome obstacles on a daily basis in order to offer the Azeri public good and reliable information. In June of 2011, Khadija Ismayilova, an investigative reporter with Radio Free Europe/Radio Liberty's (RFE/RL) Baku-based office reported that the daughters of the Azeri president, Ilham Aliyev, secretly run http://bit.ly/rferl-azerfon[a fast-rising telecom company, Azerfon] through offshore companies based in Panama. The company boasts nearly 1.7 million subscribers, covers 80 percent of the country's territory, and was (at the time) Azerbaijan's only provider of 3G services. Ismayilova spent three years trying to find out who the owners of the telecom company were, but the government refused to disclose shareholder information and lied numerous times about the company's ownership. They even claimed that the company was owned by the Germany-based Siemens AG, a claim that has been flatly denied by that corporation. The Azeri reporter managed to find out that Azerfon was owned by a few Panama-based private companies. This seemed to be a dead end to her reporting until she got help from outside. In early 2011, Ms. Ismayilova learned through the Investigative Dashboard that Panama-based companies can be tracked down through http://ohuiginn.net/panama/[an application] developed by programmer and activist Dan O'Huiginn. With this tool, she finally managed to uncover the fact that the president's two daughters were involved with the telecom company through the Panama-based businesses.

In fact, O'Huiginn created a tool that helped journalists from all over the world to report on corruption--Panama, a very well-known offshore haven, has been widely used by several corrupt officials as a place to hide stolen money (from cronies of the former Egyptian president, Hosni Mubarak to dirty officials in the Balkans or in Latin America). What the programmer-activist has done is called web scraping; a method that allows the extraction and reshaping of information so that it can be used by investigators. O'Huiginn scraped the http://www.registro-publico.gob.pa/[Panama registry of companies] because this registry, although open, only allowed searches if the investigative reporter knew the name of the commercial company he or she was looking for. This limited the possibilities of investigation, as reporters usually look for names of persons in order to track down their assets. He extracted the data and created a new website where name-based searches are also possible. The new website allowed investigative reporters in many countries to fish for information, to run names of officials in governments and Parliaments, and to check if they secretly owned corporations in Panama (just as the family of the Azerbaijan president did).

There are other advantages to using the guidelines highlighted above, besides better access to information. One of them has to do with minimizing harm and ensuring better protection for investigative reporters who work in hostile environments. This is due to the fact that when working in a network, the journalist is not alone; the investigative reporter works with colleagues in other countries, so it is harder for criminals to pass:[<phrase role='keep-together'>pinpoint</phrase>] who is responsible for their wrongdoings being exposed. As a result, retaliation by governments and corrupt officials is much harder to achieve.

Another thing to keep in mind is that information that doesn't seem very valuable in a geographical area might be crucially important in another. The exchange of information over investigative networks can lead to breaking very important stories. For example, the information that a Romanian was caught in Colombia with 1 kilogram of cocaine is most probably not front page news in Bogota, but could be very important to the Romanian public if a local reporter manages to find out that the person who was caught with the narcotics is working for the government in Bucharest.

Efficient investigative reporting is the result of cooperation between investigative journalists, programmers, and others who want to use data to contribute to create a cleaner, fairer, and more just global society.

&mdash; _Paul Radu, Organized Crime and Corruption Reporting Project_ 

=== Our Stories Come As Code 

http://www.opendatacity.de/[OpenDataCity] was founded towards the end of 2010. There was pretty much nothing that you could call data journalism happening in Germany at this time.

Why did we do this? Many times we heard people working for newspapers and broadcasters say: ``No, we are not ready to start a dedicated data journalism unit in our newsroom. But we would be happy to outsource this to someone else.''

As far as we know, we are the only company specializing exclusively in data journalism in Germany. There are currently three of us: two of us with a journalism background and one with a deep understanding of code and visualization. We work with a handful of freelance hackers, designers, and journalists.

In the last twelve months we have undertaken four data journalism projects with newspapers, and have offered training and consultancy to media workers, scientists, and journalism schools. The first app we did was TAZ, an http://bit.ly/taz-airport-noise[interactive tool on airport noise] around the newly built airport in Berlin. Our next notable project was an http://bit.ly/zeit-telephone[application about data retention] of the mobile phone usage of a German politician with ZEIT Online. For this, we won a http://bit.ly/grimme-award[Grimme Online Award] and a Lead Award in Germany, and an Online Journalism Award from the http://bit.ly/online-news-award[Online Journalism Association] in the US. At the time of writing, we have several projects in the pipeline, ranging from simpler interactive infographics up to designing and developing a kind of data journalism middleware.

[[FIG0215]]
.Airport noise map (Taz.de)
image::figs/incoming/02-TT.png[float="none"]

Of course, winning prizes helps build a reputation. But when we talk to the publishers, who have to approve the projects, our argument for investing into data journalism is not about winning prizes. Rather it is about getting attention over a longer period of time in a sustainable way. That is, building things for their long term impact, not for the scoop, which is often forgotten after a few days.

Here are three arguments that we have used to encourage publishers to undertake longer term projects:

++++
<?dbfo-need height="1in"?>
++++

Data projects don't date::
  Depending on their design, new material can be added to data journalism apps. And they are not just for the users, but can be used internally for reporting and analysis. If you're worried that this means that your competitors will also benefit from your investment, you could keep some features or some data for internal use only.
You can build on your past work::
  When undertaking a data project, you will often create bits of code that can be reused or updated. The next project might take half the time, because you know much better what to do (and what not to), and you have bits and pieces you can build on.
Data journalism pays for itself::
  Data-driven projects are cheaper than traditional marketing campaigns. Online news outlets will often invest in things like Search Engine Optimization (SEO) and Search Engine Marketing (SEM). A executed data project will normally generate a lot of clicks and buzz, and may go viral. Publishers will typically pay less for this than trying to generate the same attention by clicks and links through SEM.

Our work is not very different from other new media agencies: providing applications or services for news outlets. But maybe we differ in that we think of ourselves first and foremost as journalists. In our eyes, the products we deliver are articles or stories, albeit ones which are provided not in words and pictures, audio or video, but in code. When we are talking about data journalism, we have to talk about technology, software, devices, and how to tell a story with them.

To give an example, we just finished working on an application that pulls in realtime data via a scraper from the German railway website, thus enabling us to develop an interactive http://zugmonitor.sueddeutsche.de/[Train Monitor for Süddeutsche Zeitung], showing the delays of long-distance trains in real time. The application data is updated every minute or so, and we are providing an API for it, too. We started doing this several months ago, and have so far collected a huge dataset that grows every hour. By now it amounts to hundreds of thousands of rows of data. The project enables the user to explore this realtime data, and to do research in the archive of previous months. In the end, the story we are telling will be significantly defined by the individual action of the users.

In traditional journalism, due to the linear character of written or broadcasted media, we have to think about a beginning, the end, the story arc, and the length and angle of our piece. With data journalism things are different. There is a beginning, yes. People come to the website and get a first impression of the interface. But then they are on their own. Maybe they stay for a minute, or half an hour.

++++
<?dbfo-need height="1in"?>
++++

Our job as data journalists is to provide the framework or environment for this. As well as the coding and data management bits, we have to think of clever ways to design experiences. The User Experience (UX) derives mostly from the (Graphical) User Interface (GUI). In the end, this is the part which will make or break a project. You could have the best code working in the background handling an exciting dataset. But if the front end sucks, nobody will care about it.

There is still a lot to learn about and to experiment with. But luckily there is the games industry, which has been innovating with respect to digital narratives, ecosystems, and interfaces for several decades now. So when developing data journalism applications, we should closely watch how game design works and how stories are told in games. Why are casual games like Tetris such fun? And what makes the open worlds of sandbox games like Grand Theft Auto or Skyrim rock?

We think that data journalism is here to stay. In a few years, data journalism workflows will be quite naturally embedded in newsrooms, because news websites will have to change. The amount of data that is publicly available will keep on increasing. But luckily, new technologies will continue to enable us to find new ways of telling stories. Some of the stories will be driven by data, and many applications and services will have a journalistic character. The interesting question is which strategy newsrooms will develop to foster this process. Are they going to build up teams of data journalists integrated into their newsroom? Will there be R&D departments, a bit like in-house startups? Or will parts of the work be outsourced to specialized companies? We are still right at the beginning and only time will tell.

&mdash; _Lorenz Matzat, OpenDataCity_

=== Kaas & Mulvad: Semi-Finished Content for Stakeholder Groups

Stakeholder media is an emerging sector, largely overlooked by media theorists, which could potentially have a tremendous impact either through online networks or by providing content to news media. It can be defined as (usually online) media controlled by organizational or institutional stakeholders, which is used to advance certain interests and communities. NGOs typically create such media; so do consumer groups, professional associations, labor unions, and so on. The key limit on its ability to influence public opinion or other stakeholders is often that it lacks the capacity to undertake discovery of important information, even more so than the downsized news media. Kaas & Mulvad, a for-profit Danish corporation, is one of the first investigative media enterprises that provides expert capacity to these stakeholder outlets.

The firm originated in 2007 as a spinoff of the non-profit Danish Institute for Computer-Assisted Reporting (Dicar), which sold investigative reports to media and trained journalists in data analysis. Its founders, Tommy Kaas and Nils Mulvad, were previously reporters in the news industry. Their new firm offers what they call ``data plus journalistic insight'' (content that remains semi-finished, requiring further editing or rewriting) mainly to stakeholder media, which finalize the content into news releases or stories and distribute it through both news media and their own outlets (such as websites). Direct clients include government institutions, PR firms, labor unions, and NGOs such as EU Transparency and the World Wildlife Fund. The NGO work includes monitoring farm and fishery subsidies, and regular updates on EU lobbyist activities generated through ``scraping'' of pertinent websites. Indirect clients include foundations that fund NGO projects. The firm also works with the news industry; a tabloid newspaper purchased their celebrity monitoring service, for example.

[[FIG0216]]
.Stakeholder media companies (Fagblaget3F)
image::figs/incoming/02-MM.png[float="none"]

++++
<?dbfo-need height="1in"?>
++++

Data journalism projects in their portfolio include:

http://bit.ly/3F-unemployment[Unemployment Map for 3F]:: 
  A data visualization with key figures about unemployment in Denmark undertaken for 3F, which is the union for unskilled labor in Denmark.

http://bit.ly/3F-living[Living Conditions for 3F]::
  Another project for 3F shows how different living conditions are in different parts of Denmark. The map uses 24 different indicators.

http://bit.ly/3F-debt-index[Debt for ``Ugebrevet A4'']::
  A project that calculates a ``debt index'' and visualizes the differences in private economy.

http://bit.ly/3F-dangerous-facilities[Dangerous Facilities in Denmark]::
  A project which maps and analyzes the proximity of dangerous facilities to kindergartens and other daycare institutions, undertaken for "Børn&Unge," a magazine published by BUPL, the Danish Union of Early Childhood and Youth Educators.

http://data.vestas.com[Corporate Responsibility Data for Vestas]::
  Data visualization on five areas of CR-data for the Danish wind turbine company, Vestas, with auto-generated text. Automatically updated on a quarterly basis with 400 web pages from world scale data down to the single production unit.

http://xpoint.experian.dk/navnekort[Name Map for Experian]::
  Type in your last name and look at the distribution of this name around different geographical areas in Denmark.

http://ekstrabladet.dk/kup/fodevarer[Smiley Map for Ekstra Bladet]::
  Every day Kaas & Mulvad extract all the bad food inspections and map all the latest for the Danish tabloid Ekstra Bladet (see halfway down the website for the map).

Kaas & Mulvad are not the first journalists to work with stakeholder media. Greenpeace, for example, routinely engages journalists as collaborators for its reports. But we know of no other firm whose offerings to stakeholder media are data-driven; it is much more typical for journalists to work with NGOs as reporters, editors, or writers. The current focus in computer-assisted news media is on search and discovery (think of WikiLeaks); here again, Kaas & Mulvad innovate by focusing on data analysis. Their approach requires not only programming skills, but also an understanding of what kind of information can make a story with impact. It can safely be said that anyone who wishes to imitate their service would probably have to acquire those two skill sets through partnership, because individuals rarely possess both.

==== Processes: Innovative IT Plus Analysis

The firm undertakes about 100 projects per year, ranging in duration from a few hours to a few months. It also continuously invests in projects that expand its capacity and offerings. The celebrity monitoring service was one such experiment. Another involved scraping the Internet for news of home foreclosures and creating maps of the events. The partners say that their first criteria for projects is whether they enjoy the work and learn from it; markets are sought after a new service is defined. They make it clear that in the news industry, they found it difficult to develop new methods and new business.

Mulvad comments that:

____
We have no editors or bosses to decide which projects we can do, which software or hardware we can buy. We can buy the tools according to project needs, like the best solutions for text scraping and mining. Our goal is to be cutting edge in these areas. We try to get customers who are willing to pay, or if the project is fun we do it for a lower charge.
____

==== Value Created: Personal and Firm Brands and Revenue

Turnover in 2009 was approximately 2.5 million Danish kroner, or €336,000. The firm also sustains the partners' reputations as cutting edge journalists, which maintains demand for their teaching and speaking services. Their public appearances, in turn, support the firm's brand.

==== Key Insights of This Example

  * The news industry's crisis of declining capacity is also a crisis of under-utilization of capacity. Kaas and Mulvad had to leave the news industry to do work they valued, and that pays. Nothing prevented a news organization from capturing that value.
  * In at least some markets, there exists a profitable market for ``semi-finished'' content that can serve the interests of stakeholder groups.
  * However, this opportunity raises the issue of how much control journalists can exercise over the presentation and use of their work by third parties. We recall that this issue already exists within the news industry (where editors can impose changes on a journalist's product), and it has existed within other media industries (such as the film industry, where conflicts between directors and studios over ``final cuts'' are hardly rare). It is not a particular moral hazard of stakeholder media, but it will not disappear, either. More attention is needed to the ethics of this growing reality and market.
  * From a revenue standpoint, a single product or service is not enough. Successful watchdog enterprises would do better to take a portfolio approach, in which consulting, teaching, speaking, and other services bring in extra revenue and support the watchdog brand.

&mdash; _Edited excerpt from Mark Lee Hunter and Luk N. Van Wassenhove, ``Disruptive News Technologies: Stakeholder Media and the Future of Watchdog Journalism Business Models''. INSEAD Working Paper, 2010_

=== Business Models for Data Journalism 

Amidst all the interest and hope regarding data-driven journalism, there is one question that newsrooms are always curious about: what are the business models?

While we must be careful about making predictions, a look at the recent history and current state of the media industry can give us some insight. Today there are many news organizations who have gained by adopting new approaches.

Terms like "data journalism" and the newest buzzword, "data science," may sound like they describe something new, but this is not strictly true. Instead these new labels are just ways of characterizing a shift that has been gaining strength over decades.

Many journalists seem to be unaware of the size of the revenue that is already generated through data collection, data analytics, and visualization. This is the business of information refinement. With data tools and technologies, it is increasingly possible to shed light on highly complex issues, be this international finance, debt, demography, education, and so on. The term "business intelligence" describes a variety of IT concepts that aim to provide a clear view on what is happening in commercial corporations. The big and profitable companies of our time, including McDonalds, Zara, and H&M, rely on constant data tracking to turn out a profit. And it works pretty well for them.

What is changing right now is that the tools developed for this space are now becoming available for other domains, including the media. And there are journalists who get it. Take Tableau, a company that provides a suite of visualization tools. Or the ``Big Data'' movement, where technology companies use (often open source) software packages to dig through piles of data, extracting insights in milliseconds.

These technologies can now be applied to journalism. Teams at the Guardian and The New York Times are constantly pushing the boundaries in this emerging field. And what we are currently seeing is just the tip of the iceberg.

But how does this generate money for journalism? The big, worldwide market that is currently opening up is all about transformation of publicly available data into something our that we can process: making data visible and making it human. We want to be able to relate to the big numbers we hear every day in the news--what the millions and billions mean for each of us.

There are a number of very profitable data-driven media companies, who have simply applied this principle earlier than others. They enjoy healthy growth rates and sometimes impressive profits. One example is Bloomberg. The company operates about 300,000 terminals and delivers financial data to its users. If you are in the money business, this is a power tool. Each terminal comes with a color-coded keyboard and up to 30,000 options to look up, compare, analyze, and help you to decide what to do next. This core business generates an estimated $6.3 billion (US) per year--at least as estimated in http://nyti.ms/IQcRgY[a 2008 piece by The New York Times]. As a result, Bloomberg has been hiring journalists left, right and center, they bought the venerable but loss-making ``Business Week,'' and so on.

Another example is the Canadian media conglomerate today known as Thomson Reuters. They started with one newspaper, bought up a number of well-known titles in the UK, and then decided two decades ago to leave the newspaper business. Instead, they have grown based on information services, aiming to provide a deeper perspective for clients in a number of industries. If you worry about how to make money with specialized information, my advice would be to just http://en.wikipedia.org/wiki/The_Thomson_Corporation[read about the company's history on Wikipedia].

And look at the Economist. The magazine has built an excellent, influential brand on its media side. At the same time, the ``Economist Intelligence Unit'' is now more like a consultancy, reporting about relevant trends and forecasts for almost any country in the world. They are employing hundreds of journalists and claim to serve about 1.5 million customers worldwide.

And there are many niche data-driven services that could serve as inspiration: eMarketer in the US, providing comparisons, charts, and advice for anybody interested in internet marketing; Stiftung Warentest in Germany, an institution looking into the quality of products and services; Statista, again from Germany, a start-up helping to visualize publicly available information.

Around the world, there is currently a wave of startups in this sector, naturally covering a wide range of areas; for example, Timetric, which aims to ``reinvent business research,'' OpenCorporates, Kasabi, Infochimps, and Data Market. Many of these are arguably experiments, but together they can be taken as an important sign of change.

Then there is the public media, which in terms of data-driven journalism, is a sleeping giant. In Germany, €7.2 billion per year is flowing into this sector. Journalism is a special product: if done well, it is not just about making money, but serves an important role in society. Once it is clear that data journalism can provide better, more reliable insights more easily, some of this money could be used for new jobs in newsrooms.

With data journalism, it is not just about being first, but about being a trusted source of information. In this multichannel world, attention can be generated in abundance, but _trust_ is an increasingly scarce resource. Data journalists can help to collate, synthesize, and present diverse and often difficult sources of information in a way that gives their audience real insights into complex issues. Rather than just recycling press releases and retelling stories they've heard elsewhere, data journalists can give readers a clear, comprehensible, and preferably customizable perspective with interactive graphics and direct access to primary sources. Not trivial, but certainly valuable.

So what is the best approach for aspiring data journalists to explore this field and convince management to support innovative projects?

++++
<?dbfo-need height="1in"?>
++++

The first step should be to look for immediate opportunities close to home: low-hanging fruit. For example, you might already have collections of structured texts and data that you could use. A prime example of this is the ``Homicide database'' of the Los Angeles Times. Here, data and visualizations are the core, not an afterthought. The editors collect all the crimes they find and only then write articles based on this. Over time, such collections are becoming better, deeper, and more valuable.

This might not work the first time. But it will over time. One very hopeful indicator here is that the Texas Tribune and ProPublica, which are both arguably post-print media companies, reported that funding for their non-profit journalism organizations exceeded their goals much earlier than planned.

Becoming proficient in all things data--whether as a generalist or as a specialist focused on one aspect of the data food chain--provides a valuable perspective for people who believe in journalism. As one well-known publisher in Germany recently said in an interview, ``There is this new group who call themselves data journalists. And they are not willing to work for peanuts anymore.''

&mdash; _Mirko Lorenz, Deutsche Welle_
